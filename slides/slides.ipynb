{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"A reinforcement learning perspective on industrial model predictive control\" \n",
        "subtitle: \"Upper Bound 2024\"\n",
        "author: \"Nathan Lawrence\"\n",
        "format: \n",
        "    html:\n",
        "        html-table-processing: none\n",
        "        embed-resources: true\n",
        "        auto-play-media: true\n",
        "    revealjs:\n",
        "        slide-number: c\n",
        "        show-slide-number: all\n",
        "        theme: [default, custom.scss]\n",
        "        auto-play-media: true\n",
        "        embed-resources: true\n",
        "bibliography: tutorial.bib\n",
        "---"
      ],
      "id": "4e895eb8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Motivation -- MPC, RL solving same thing)\n",
        "\n",
        "![](figs/rl_drawing.jpg)\n",
        "\n",
        "![](figs/mpc_drawing.jpg)\n",
        "\n",
        "##\n",
        "\n",
        "\n",
        "\n",
        "```{tikz complete-pooling, engine.opts=font_opts}\n",
        "#| echo: false\n",
        "#| fig-cap: \"Complete pooling\"\n",
        "#| fig-align: center\n",
        "#| fig-ext: svg\n",
        "#| out-width: 100%\n",
        "\\usetikzlibrary{positioning}\n",
        "\\usetikzlibrary{shapes.geometric}\n",
        "\\begin{tikzpicture}[{every node/.append style}=draw]\n",
        "  \\node [rectangle] (pop) at (0, 4) {Population};\n",
        "  \\node [ellipse] (y1) at (-5, 2.5) {$y_1$};\n",
        "  \\node [ellipse] (y2) at (-3, 2.5) {$y_2$};\n",
        "  \\node [ellipse] (y3) at (-1, 2.5) {$y_3$};\n",
        "  \\node [ellipse] (y4) at (1, 2.5) {$y_4$};\n",
        "  \\node [ellipse, draw=white] (ydots) at (3, 2.5) {$\\dots$};\n",
        "  \\node [ellipse] (yn) at (5, 2.5) {$y_n$};\n",
        "  \\draw [-latex] (pop) to (y1);\n",
        "  \\draw [-latex] (pop) to (y2);\n",
        "  \\draw [-latex] (pop) to (y3);\n",
        "  \\draw [-latex] (pop) to (y4);\n",
        "  \\draw [-latex, dashed] (pop) to (ydots);\n",
        "  \\draw [-latex] (pop) to (yn);\n",
        "\\end{tikzpicture}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## (Today -- combine so they complement each other)\n",
        "\n",
        "## Today\n",
        "\n",
        "- RL, MPC, and some stuff in-between pertaining to process control\n",
        "- How to implement these ideas\n",
        "\n",
        "- Emphasis on intuition rather than rigor\n",
        "- Ask questions and be ready to discuss things with your neighbor\n",
        "\n",
        "I hope this works for you!\n",
        "\n",
        "\n",
        "# Reinforcement learning\n",
        "\n",
        "## Agents and environments {.center}\n",
        "\n",
        "## Environment\n",
        "\n",
        "$s_t$ -- state\n",
        "\n",
        "$a_t$ -- action\n",
        "\n",
        "Actions and states lead to new states $$s_{t+1} \\sim p \\left( s_{t+1} \\middle| s_t, a_t \\right)$$\n",
        "\n",
        "![](figs/mdp.png){width=\"800\" fig-align=\"center\"}\n",
        "\n",
        "## Environment\n",
        "\n",
        "::: {layout=\"[[0.6,1,1]]\" layout-valign=\"center\"}\n",
        "Apply torque\n",
        "\n",
        "![](figs/pendulum.gif){height=350 fig-align=\"center\"}\n",
        "\n",
        "<div>\n",
        "\n",
        "Observe\n",
        "\n",
        "-   Angle\n",
        "-   Angular velocity\n",
        "\n",
        "</div>\n",
        ":::\n",
        "\n",
        "## Environment\n",
        "\n",
        "Continuing forever gives a trajectory $$s_0, a_0, s_1, a_1, \\ldots, s_{t}, a_{t}, s_{t+1}, \\ldots$$\n",
        "\n",
        "Most trajectories are \"bad\"\n",
        "\n",
        "We want a system that discovers \"good\" behavior in the environment\n",
        "\n",
        "![](figs/blunder.png){width=\"250\" fig-align=\"center\"}\n",
        "\n",
        "## Agent\n",
        "\n",
        "The agent is the learner:\n",
        "\n",
        "-   Decides which actions to take\n",
        "-   Improves its behavior\n",
        "\n",
        "![](figs/RL_illustration.png){fig-align=\"center\"}\n",
        "\n",
        "::: aside\n",
        "Figure: @weng2018bandit\n",
        ":::\n",
        "\n",
        "## Agent\n",
        "\n",
        "**Reward** guides learning:\n",
        "\n",
        "-   A scalar feedback signal\n",
        "-   Indicates which states & actions are good\n",
        "\n",
        "![](figs/RLSutton.png){height=\"250\" fig-align=\"center\"}\n",
        "\n",
        "::: aside\n",
        "Figure: @sutton2018ReinforcementLearning\n",
        ":::\n",
        "\n",
        "## What are we really trying to solve? {.center}\n",
        "\n",
        "## \n",
        "\n",
        "The agent-environment interface yields the trajectory $$s_0, a_0, r_0, s_1, a_1, r_1 \\ldots, s_{t}, a_{t}, r_{t}, s_{t+1}, \\ldots$$\n",
        "\n",
        "States governed by $$s_{t+1} \\sim p \\left( s_{t+1} \\middle| s_t, a_t \\right)$$ Agent chooses actions from a **policy** $$a_t \\sim \\pi\\left( a_t \\middle| s_t \\right)$$ Rewards assigned by a function $$r_t = r(s_t, a_t)$$\n",
        "\n",
        "## The space of policies is vast\n",
        "\n",
        "-   Completely random\n",
        "-   An industrial control module\n",
        "-   Neural network\n",
        "\n",
        "::: r-stack\n",
        "![](figs/tmp.png){.fragment height=\"230\" fig-align=\"center\"}\n",
        "\n",
        "![](figs/Honey_UDC.jpg){.fragment height=\"250\" fig-align=\"center\"}\n",
        "\n",
        "![](figs/nn.png){.fragment height=\"270\" fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: {.fragment .r-fit-text}\n",
        "We want the \"best\" policy!\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "-   Take $a \\in \\text{argmax}_{a} \\left\\{ r(s,a) \\right\\}$?\n",
        "\n",
        "::: r-stack\n",
        "$\\rightarrow$ too shortsighted\n",
        ":::\n",
        "\n",
        "-   Maximize $r_0 + r_1 + r_2 + \\ldots$?\n",
        "\n",
        "::: r-stack\n",
        "$\\rightarrow$ might diverge\n",
        ":::\n",
        "\n",
        "Consider the discounted **return** $$G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\ldots$$ where $\\gamma \\in [0,1]$\n",
        "\n",
        "## \n",
        "\n",
        "-   Typically, $\\gamma \\in (0,1)$\n",
        "-   Assume $r$ is bounded by $\\bar{r}$\n",
        "\n",
        "Return is bounded across all possible trajectories $$\\lvert G_t \\rvert \\leq \\frac{\\bar{r}}{1 - \\gamma}$$\n",
        "\n",
        "![](figs/geometric.png){height=\"200\" fig-align=\"center\"}\n",
        "\n",
        "\n",
        "::: aside\n",
        "[Geometric series (Wikipedia)](https://en.wikipedia.org/wiki/Geometric_series)\n",
        ":::\n",
        "\n",
        "## The reinforcement learning problem {auto-animate=\"true\"}\n",
        "\n",
        "::: {style=\"margin-top: 200px;\"}\n",
        "$$ \\text{maximize} \\quad J(\\pi) = \\mathbb{E}_{\\pi}\\left[ \\sum_{t=0}^{\\infty} \\gamma^{t}r(s_t,a_t) \\right]$$\n",
        ":::\n",
        "\n",
        "## The reinforcement learning problem {auto-animate=\"true\"}\n",
        "\n",
        "$$ \\text{maximize} \\quad J(\\pi) = \\mathbb{E}_{\\pi}\\left[ \\sum_{t=0}^{\\infty} \\gamma^{t}r(s_t,a_t) \\right]$$\n",
        "\n",
        "Why is this hard/impossible?\n",
        "\n",
        "::: incremental\n",
        "-   Infinite horizon\n",
        "-   Search space\n",
        "-   Randomness\n",
        "-   No system description\n",
        ":::\n",
        "\n",
        "## Abstracting the objective through value functions {.center}\n",
        "\n",
        "## \n",
        "\n",
        "What if we had an optimal trajectory? $$s_0^\\star, a_0^\\star, r_0^\\star, s_1^\\star, a_1^\\star, r_1^\\star \\ldots, s_{t}^\\star, a_{t}^\\star, r_{t}^\\star, s_{t+1}^\\star, \\ldots$$\n",
        "\n",
        "::: {layout=\"[[1,0.5]]\"}\n",
        "Then the trajectory starting at $s_t^\\star$ should still be optimal $$s_{t}^\\star, a_{t}^\\star, r_{t}^\\star, s_{t+1}^\\star, \\ldots$$\n",
        "\n",
        "![](figs/subpath.png)\n",
        ":::\n",
        "\n",
        "::: aside\n",
        "[Optimal substructure (Wikipedia)](https://en.wikipedia.org/wiki/Optimal_substructure)\n",
        ":::\n",
        "\n",
        "## Chess example\n",
        "\n",
        "(placeholder)\n",
        "\n",
        "## \n",
        "\n",
        "-   Define the **state-action value** to be $$Q(s,a) = \\mathbb{E}_{\\pi}\\left[ \\sum_{t=0}^{\\infty} \\gamma^{t}r(s_t,a_t) | s_0 = s, a_0 = a \\right]$$\n",
        "-   Similarly, the (state) **value** averages over the policy: \n",
        "\\begin{align}\n",
        "V(s) &= \\mathbb{E}_{\\pi}\\left[ \\sum_{t=0}^{\\infty} \\gamma^{t}r(s_t,a_t) | s_0 = s \\right]\\\\\n",
        "&= \\mathbb{E}_{a \\sim \\pi(a | s)}\\left[ Q(s,a) \\right]\n",
        "\\end{align}\n",
        "\n",
        "##  {.center}\n",
        "\n",
        "Therefore: $$J(\\pi) = \\mathbb{E}_{s_0 \\sim p(s_0)}\\left[ V(s_0) \\right]$$\n",
        "\n",
        "## What's the big deal with these magical functions?\n",
        "\n",
        "Observe \n",
        "\\begin{align}\n",
        "G_t &= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\ldots \\\\\n",
        "&= r_t + \\gamma \\left( r_{t+1} + \\gamma r_{t+2} + \\ldots \\right) \\\\\n",
        "&= r_t + \\gamma G_{t+1}\n",
        "\\end{align}\n",
        "\n",
        "Compresses an infinite number of actions into a simple scalar recursion!\n",
        "\n",
        "## What's the big deal with these magical functions?\n",
        "\n",
        "Averaging...\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "V(s) &= \\mathbb{E}_{\\pi} \\left[ G_t \\middle| s_t = s \\right] \\\\\n",
        "&= \\mathbb{E}_{\\pi} \\left[ r_t + \\gamma G_{t+1} \\middle| s_t = s \\right] \\\\\n",
        "&= \\mathbb{E}_{a \\sim \\pi(a | s)} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s' | s, a)} \\left[ V(s') \\right]\\right]\n",
        "\\end{align}\n",
        "\n",
        "Similarly for $Q$...\n",
        "\n",
        "\\begin{align}\n",
        "Q(s,a) &= r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s' | s, a)}\\left[ V(s') \\right]\\\\\n",
        "&= r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s' | s, a), a' \\sim \\pi(a' | s')}\\left[ Q(s',a') \\right]\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "## What's the big deal with these magical functions?\n",
        "\n",
        "Graphically...\n",
        "\n",
        "::: {layout=\"[[1,1],[1,1]]\"}\n",
        "$V(s)$\n",
        "\n",
        "$Q(s,a)$\n",
        "\n",
        "![](figs/backup_v.png){height=\"50%\"}\n",
        "\n",
        "![](figs/backup_q.png){height=\"50%\"}\n",
        ":::\n",
        "\n",
        "These are the **Bellman equations** for $V$ and $Q$\n",
        "\n",
        "## What's the big deal with these magical functions?\n",
        "\n",
        "-   The same function appears on both sides of the equation!\n",
        "\n",
        "$$V(s) = \\mathbb{E}\\left[ r(s,a) + \\gamma V(s') \\right]$$\n",
        "\n",
        "-   Will enable learning based on single-step transition data\n",
        "\n",
        "::: {.fragment .r-fit-text}\n",
        "Bellman equation holds for any policy!\n",
        ":::\n",
        "\n",
        "## Bellman optimality equation {.center}\n",
        "\n",
        "An optimal policy $\\pi^\\star$ solves the following:\n",
        "\n",
        "::: {layout=\"[[1,1]]\"}\n",
        "$$V^\\star (s) = \\max_{\\pi} V(s)$$\n",
        "\n",
        "$$Q^\\star (s,a) = \\max_{\\pi} Q(s,a)$$\n",
        ":::\n",
        "\n",
        "We don't actually have $\\pi^\\star$, but it's fun to imagine...\n",
        "\n",
        "## Bellman optimality equation\n",
        "\n",
        "Given $V^\\star$...\n",
        "\n",
        "\\begin{align}\n",
        "V^\\star (s) &= \\mathbb{E}_{a \\sim \\pi^\\star (a | s)} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s' | s, a)} \\left[ V^\\star (s') \\right]\\right]\\\\\n",
        "&= \\max_{a}\\left\\{ r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s' | s, a)} \\left[ V^\\star (s') \\right] \\right\\}\n",
        "\\end{align}\n",
        "\n",
        "...One-step search!\n",
        "\n",
        "::: {.r-fit-text}\n",
        "Short-term planning is long-term optimal\n",
        ":::\n",
        "\n",
        "## Bellman optimality equation\n",
        "\n",
        "Given $Q^\\star$...\n",
        "$$V^\\star (s) = \\max_{a} Q^\\star (s,a)$$\n",
        "\n",
        "...Even easier!\n",
        "\n",
        "::: {.r-fit-text}\n",
        "Learn $Q^\\star$ and then maximize it!\n",
        ":::\n",
        "\n",
        "## Evaluate, improve, repeat... {.center}\n",
        "\n",
        "## \n",
        "\n",
        "Like $V^\\star$, $Q^\\star$ satisfies a neat optimality condition:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "Q^\\star (s,a) &= r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s' | s, a), a' \\sim \\pi^\\star (a' | s')}\\left[ Q^\\star (s',a') \\right] \\\\\n",
        "&= r(s,a) + \\gamma  \\mathbb{E}_{s' \\sim p(s' | s, a)}\\left[ \\max_{a'} Q^\\star (s',a') \\right]\n",
        "\\end{align}\n",
        "\n",
        "\"Plug this magical function $Q^\\star$ into the RHS produces the same function\"\n",
        "\n",
        "## Fixed-point iteration\n",
        "\n",
        "Iterating $q^{k+1} = B(q^k)$ **may** converge to some $q$ where $q = B(q)$\n",
        "\n",
        "![](figs/fixedpoint.png)\n",
        "\n",
        "##  {.center}\n",
        "\n",
        "-   Let's just take the operator $$B(Q) = r(s,a) + \\gamma  \\mathbb{E}_{s' \\sim p(s' | s, a)}\\left[ \\max_{a'} Q (s',a') \\right]$$ and apply fixed-point iteration!\n",
        "\n",
        "-   The RHS is an **idealized** update scheme and key source of inspiration\n",
        "\n",
        "## Fixed-point aspirations\n",
        "\n",
        "If we have a policy $\\pi$ and some oracle that tells us $Q$, take \n",
        "\\begin{align}\n",
        "\\pi^{+}(s) &= \\text{arg}\\max\\limits_{a} Q(s,a) \\\\\n",
        "&= \\text{arg}\\max\\limits_{a} \\left\\{ r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s' | s, a)} \\left[ V (s') \\right] \\right\\}\n",
        "\\end{align}\n",
        "\n",
        "Then $\\pi^{+}$ is at least as good as $\\pi$! That is, $Q^{+} = B(Q) \\geq Q$.\n",
        "\n",
        "::: aside\n",
        "There are many ways to proceed: Policy iteration, value iteration, and we haven't even mentioned policy evaluation. We're focusing on high-level ideas and avoiding introducing jargon, so see @sutton2018ReinforcementLearning for a full treatment.\n",
        ":::\n",
        "\n",
        "##  {visibility=\"hidden\"}\n",
        "\n",
        "$$\\pi^{+}(s) = \\bbox[#DCD0FF,2pt]{\\text{arg}\\max\\limits_{a}} \\left\\{ r(s,a) + \\gamma \\bbox[#FF9999,2pt]{\\mathbb{E}} \\left[ \\bbox[#AAF0D1,2pt]{V (s')} \\right] \\right\\}$$\n",
        "\n",
        "## Three important approximations\n",
        "\n",
        "![](figs/pi_update.png){fig-align=\"center\"}\n",
        "\n",
        "::: aside\n",
        "See @bertsekas2023course\n",
        ":::\n",
        "\n",
        "##  {.center}\n",
        "\n",
        "::: {layout=\"[[1],[1]]\"}\n",
        "To move things along we will work through some examples instead of focusing on the minutiae of common RL algorithms\n",
        "\n",
        "![](figs/rl_algs.png){fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "::: aside\n",
        "Figure: [Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html)\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "# Examples\n",
        "\n",
        "## Learning to balance {.center}\n",
        "\n",
        "## Cartpole\n",
        "\n",
        "::: {layout=\"[[65,35]]\" layout-valign=\"center\"}\n",
        "\n",
        "![](figs/cart_pole.gif){height=\"90%\"}\n",
        "\n",
        "**Don't fall!**\n",
        ":::\n",
        "\n",
        "## Deep Q-networks (DQNs)\n",
        "\n",
        "\n",
        "For a **finite** set of actions, define the neural network\n",
        "$$\\text{DQN}(s) =\n",
        "\\begin{bmatrix}\n",
        "q_1 \\\\\n",
        "\\vdots \\\\\n",
        "q_d\n",
        "\\end{bmatrix}$$ \n",
        "where each $q_i$ is an approximation of $Q(s,a_i)$\n",
        "\n",
        "Optimization is trivial: \n",
        "\\begin{align}\n",
        "\\pi(s) &= \\arg\\max \\text{DQN}(s) \\\\\n",
        "&= \\arg\\max\\left\\{ q_1, \\ldots, q_d \\right\\} \\\\\n",
        "&\\approx \\arg\\max_{a} Q(s,a_i)\n",
        "\\end{align}\n",
        "\n",
        "::: aside\n",
        "@mnih2013PlayingAtari\n",
        ":::\n",
        "\n",
        "##  {.center}\n",
        "\n",
        "::: {layout=\"[1], [1]\"}\n",
        "![](figs/mnih_atari.png){fig-align=\"center\" width=\"60%\"}\n",
        "\n",
        "![](figs/mnih_algo.png){fig-align=\"center\" width=\"65%\"}\n",
        ":::\n",
        "\n",
        "::: aside\n",
        "Equation 3: Approximately respect the Bellman equation $[q_1, \\ldots, q_d] \\approx r(s,a) + \\gamma \\max\\{ q_1', \\ldots q_d'\\}$\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "::: {layout=\"[[1,1], [1,1]]\"}\n",
        "![Initial](videos/dqn/episode-398.gif){width=\"80%\"}\n",
        "\n",
        "![After some time...](videos/dqn/episode-6965.gif){width=\"80%\"}\n",
        "\n",
        "![Almost there...](videos/dqn/episode-8159.gif){width=\"80%\"}\n",
        "\n",
        "![Final](videos/dqn/episode-8358.gif){width=\"80%\"}\n",
        ":::\n",
        "\n",
        "## Acrobot {auto-animate=\"true\"}\n",
        "\n",
        "::: {layout=\"[[65,35]]\" layout-valign=\"center\"}\n",
        "![](figs/acrobot.gif)\n",
        "\n",
        "<div>\n",
        "\n",
        "**Touch the line!**\n",
        "\n",
        "</div>\n",
        ":::\n",
        "\n",
        "## Acrobot {auto-animate=\"true\"}\n",
        "\n",
        "::: {layout=\"[[65,35]]\" layout-valign=\"center\"}\n",
        "![](figs/acrobot.gif)\n",
        "\n",
        "<div>\n",
        "\n",
        "~~**Touch the line!**~~\n",
        "\n",
        "**Stay above the line!**\n",
        "\n",
        "</div>\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "Applying DQN...\n",
        "\n",
        "::: {layout=\"[[1,1], [1,1]]\"}\n",
        "![Initial](videos/dqn/acrobot-dqn__0/episode-0.mp4){width=\"50%\"}\n",
        "\n",
        "![After some time...](videos/dqn/acrobot-dqn__0/episode-796.mp4){width=\"50%\"}\n",
        "\n",
        "![Almost there...](videos/dqn/acrobot-dqn__0/episode-1592.mp4){width=\"50%\"}\n",
        "\n",
        "![Final](videos/dqn/acrobot-dqn__0/episode-1990.mp4){width=\"50%\"}\n",
        ":::\n",
        "\n",
        "::: fragment\n",
        "Standing upright would be better than all of these...\n",
        ":::\n",
        "\n",
        "## Finer control requires continuous actions\n",
        "\n",
        "What we want: \n",
        "$$\n",
        "\\pi^\\star (s) = \\arg \\max_{a} Q^\\star (s,a)\n",
        "$$\n",
        "\n",
        "What DQN delivers:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\pi(s) &= \\arg \\max \\left\\{ q_1, \\ldots, q_d \\right\\} \\\\\n",
        "&\\approx \\arg \\max \\left\\{ Q^\\star(s, a_1), \\ldots, Q^\\star(s, a_d)\\right\\}\n",
        "\\end{align}\n",
        "\n",
        "## Finer control requires continuous actions\n",
        "\n",
        "Consider two separate networks:\n",
        "\n",
        "-   Policy $\\pi_{\\theta}$ (aka \"actor\")\n",
        "-   $Q$-network $Q_\\varphi$ (aka \"critic\")\n",
        "\n",
        "Idea: Use $Q_\\varphi$ as a loss function for $\\pi_{\\theta}$: \n",
        "$$\\text{maximize}_{\\theta} \\quad \\mathbb{E} \\left[ Q_\\varphi(s, \\pi_\\theta (s)) \\right]$$\n",
        "\n",
        "##  {.center}\n",
        "\n",
        "Then $$\\max_{a} Q(s,a) \\approx Q(s, \\pi_{\\theta} (s))$$\n",
        "\n",
        "::: {.r-fit-text}\n",
        "An easy-to-evaluate approximation of the argmax operation!\n",
        ":::\n",
        "\n",
        "## Disclaimer\n",
        "\n",
        "This doesn't actually work.\n",
        "\n",
        "See the DDPG, TD3, SAC papers for all the tricks/hacks that make this idea work.\n",
        "\n",
        "-   Replay buffers\n",
        "-   Target networks\n",
        "-   Exploration / exploitation\n",
        "-   Double $Q$-learning\n",
        "-   Delayed updates\n",
        "-   Smoothing\n",
        "-   Etc...\n",
        "\n",
        "::: aside\n",
        "DDPG [@lillicrap2015ContinuousControl], TD3 [@fujimoto2018AddressingFunction], SAC [@haarnoja2019SoftActorCritic]\n",
        ":::\n",
        "\n",
        "## Acrobot cont'd {.center}\n",
        "\n",
        "Let's assume we have some RL oracle: Given an environment and sufficiently powerful networks, it does a reasonable job at solving the principal RL problem\n",
        "\n",
        "\n",
        "We isolate two components:\n",
        "\n",
        "-   Reward function\n",
        "-   Discount factor $\\gamma$\n",
        "\n",
        "## Try to formulate a reward function\n",
        "\n",
        "::: {layout=\"[[1,1]]\" layout-valign=\"center/\"}\n",
        "![](figs/acrobot.png){height=\"300\"}\n",
        "\n",
        "![](figs/dp.gif){height=\"300\"}\n",
        ":::\n",
        "\n",
        "::: aside\n",
        "[Double pendulum \\[Wikipedia\\]](https://en.wikipedia.org/wiki/Double_pendulum)\n",
        ":::\n",
        "\n",
        "## Reward functions\n",
        "\n",
        "-   Default: $0$ if above line; $-1$ otherwise\n",
        "-   $\\ell^2$: Negative $2$-norm of\n",
        "    -   normalized velocities\n",
        "    -   $-1 - \\cos( q_1 )$, $1 - \\cos( q_2 )$\n",
        "-   Height\n",
        "-   $\\ell^\\infty$: Negative $\\infty$-norm of\n",
        "    -   Normalized velocities\n",
        "    -   Normalized height\n",
        "\n",
        "## Default reward\n",
        "\n",
        "![](figs/reward_default.png)\n",
        "\n",
        "## $\\ell^2$ reward\n",
        "\n",
        "![](figs/reward_l2.png)\n",
        "\n",
        "## \"Height\" reward\n",
        "\n",
        "![](figs/reward_height.png)\n",
        "\n",
        "## $\\ell^\\infty$ reward\n",
        "\n",
        "![](figs/reward_linf.png)\n",
        "\n",
        "## Summary, $\\gamma = 0.99$ {visibility=\"hidden\"}\n",
        "\n",
        "![](figs/reward_gamma_99_all.png)\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "::: {layout=\"[[1,1,1,1,1], [1,1,1,1,1], [1,1,1,1,1]]\"}\n",
        "<div>\n",
        "\n",
        "</div>\n",
        "\n",
        "default\n",
        "\n",
        "$\\ell^2$\n",
        "\n",
        "height\n",
        "\n",
        "$\\ell^\\infty$\n",
        "\n",
        "<div>\n",
        "\n",
        "\\\n",
        "\\\n",
        "$\\gamma = 0.90$\n",
        "\n",
        "</div>\n",
        "\n",
        "![](videos/gamma_90/acrobot-sac__default__1/episode-1950.mp4)\n",
        "\n",
        "![](videos/gamma_90/acrobot-sac__l2__1/episode-1950.mp4)\n",
        "\n",
        "![](videos/gamma_90/acrobot-sac__height__3/episode-1950.mp4)\n",
        "\n",
        "![](videos/gamma_90/acrobot-sac__linf__1/episode-1950.mp4)\n",
        "\n",
        "<div>\n",
        "\n",
        "\\\n",
        "\\\n",
        "$\\gamma = 0.99$\n",
        "\n",
        "</div>\n",
        "\n",
        "![](videos/gamma_99/acrobot-sac__default__0/episode-1950.mp4)\n",
        "\n",
        "![](videos/gamma_99/acrobot-sac__l2__2/episode-1950.mp4)\n",
        "\n",
        "![](videos/gamma_99/acrobot-sac__height__1/episode-1950.mp4)\n",
        "\n",
        "![](videos/gamma_99/acrobot-sac__linf__2/episode-1950.mp4)\n",
        ":::\n",
        "\n",
        "::: aside\n",
        "1 of $4^8$ configurations. Sampled via [random.org](https://www.random.org).\n",
        ":::\n",
        "\n",
        "## Bloopers\n",
        "\n",
        "I had to modify the [default environment]((https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/acrobot.py)):\n",
        "\n",
        "Action space:\n",
        "\n",
        "-   Old: $\\{-1, 0, 1\\}$ (discrete)\n",
        "-   Intermediate: $[-1, 1]$ (continuous, restricted)\n",
        "-   New: $[-2, 2]$ (continuous, expanded)\n",
        "\n",
        "Sampling time:\n",
        "\n",
        "-   Old: $dt = 0.2$ seconds\n",
        "-   New: $dt = 0.1$ seconds\n",
        "\n",
        "Plus the new reward functions\n",
        "\n",
        "## Restricted action space\n",
        "\n",
        "\"Height\" reward, $\\gamma = 0.99$, 5hz\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](videos/misc/5hz-smallTorque_gamma99_acrobat-sac__height__0/episode-1950.mp4)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/reward_blooper_height_gamma_99.png){width=\"100%\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Restricted action space cont'd\n",
        "\n",
        "$\\ell^\\infty$ reward, $\\gamma = 0.95$, 5hz\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](videos/misc/5hz-smallTorque_gamma95_acrobat-sac__linf__0/episode-1950.mp4)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/reward_blooper_linf_gamma_95.png){width=\"100%\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "## An analytic solution {.center}\n",
        "\n",
        "## What if we know something about the environment? {auto-animate=\"true\"}\n",
        "\n",
        "Let's grossly simplify the problem:\n",
        "\n",
        "$$\\text{maximize } \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t,a_t)  \\right]$$\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "-   Unknown dynamics\n",
        "-   Unstructured policy\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "-   Possibly unknown reward\n",
        "-   Everything is random\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Aside: Some notation\n",
        "\n",
        "## What if we know something about the environment? {auto-animate=\"true\"}\n",
        "\n",
        "Let's grossly simplify the problem:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\text{minimize } && \\sum_{t=0}^{\\infty} \\gamma^t \\left( x_t^2 + \\rho u_t^2 \\right)\\\\\n",
        "\\text{where } && x_{t+1} = a x_t + b u_t \\\\\n",
        "&& u_t = -k x_t\n",
        "\\end{align}\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "-   Linear, scalar dynamics\n",
        "-   Linear policy\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "-   Quadratic cost\n",
        "-   Deterministic\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Quadratic value function\n",
        "\n",
        "Trajectories are easy to compute:\n",
        "\n",
        "::: {layout=\"[[1,0.2,1]]\"}\n",
        "\\begin{align}\n",
        "x_{t+1} &= a x_t + b u_t\\\\\n",
        "u_t &= -k x_t\n",
        "\\end{align}\n",
        "\n",
        "$$\\implies$$\n",
        "\n",
        "\\begin{align}\n",
        "x_{t+1} &= \\left( a - bk \\right)x_t \\\\\n",
        "&\\vdots\\\\\n",
        "x_{t+1} &= \\left( a - bk \\right)^{t+1} x_0 \\\\\n",
        "u_t &= -k \\left( a - bk \\right)^t x_0\n",
        "\\end{align}\n",
        ":::\n",
        "\n",
        "## Quadratic value function\n",
        "\n",
        "Returns are easy to compute:\n",
        "\n",
        "\\begin{align}\n",
        "\\sum_{t=0}^{\\infty} \\gamma^t \\left( x_t^2 + \\rho u_t^2 \\right) &= x_0^2 + \\gamma (a-bk)^2 x_0^2 + \\gamma^2 (a-bk)^4 x_0^2 + \\ldots\\\\\n",
        "&+ \\rho k^2 x_0^2 + \\gamma \\rho k^2 (a-bk)^2 x_0^2 + \\ldots \\\\\n",
        "&= \\sum_{t=0}^{\\infty} \\gamma^t (a-bk)^{2t} (1 + \\rho k^2) x_0^2\n",
        "\\end{align}\n",
        "\n",
        "::: aside\n",
        "\\begin{align}\n",
        "x_{t+1} &= \\left( a - bk \\right)^{t+1} x_0\\\\\n",
        "u_t &= -k \\left( a - bk \\right)^t x_0\n",
        "\\end{align}\n",
        ":::\n",
        "\n",
        "## Quadratic value function\n",
        "\n",
        "By properties of geometric series:\n",
        "\n",
        "\\begin{align}\n",
        "\\text{return} &= \\sum_{t=0}^{\\infty} \\gamma^t (a-bk)^{2t} (1 + \\rho k^2) x_0^2 \\\\\n",
        "&= \\frac{1 + \\rho k^2}{1 - \\gamma\\left(a - bk\\right)^2} x_0^2\n",
        "\\end{align}\n",
        "\n",
        "::: aside\n",
        "$\\sum_{i = 0}^{\\infty} \\alpha \\beta^i = \\alpha \\frac{1}{1 - \\beta}$\n",
        ":::\n",
        "\n",
        "## Quadratic value function\n",
        "\n",
        "A 1-D optimization problem in $K$:\n",
        "\n",
        "::: {layout=\"[[1,1]]\" layout-valign=\"bottom\"}\n",
        "$$\\min_{K} \\frac{1 + \\rho K^2}{1 - \\gamma\\left(a - bK\\right)^2}$$\n",
        "\n",
        "![](figs/desmos-controller.png){width=\"60%\"}\n",
        ":::\n",
        "\n",
        "::: aside\n",
        "[\\[To graph\\]](https://www.desmos.com/calculator/vgnchtuqys)\n",
        ":::\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "::: {layout=\"[[1,1], [1,1], [1,1]]\"}\n",
        "$\\gamma = 1$\n",
        "\n",
        "$\\gamma = 0$\n",
        "\n",
        "![](figs/desmos-controller.png){width=\"50%\"}\n",
        "\n",
        "![](figs/desmos-gamma0-controller.png){width=\"50%\"}\n",
        "\n",
        "![](figs/desmos-stable.png){width=\"50%\"}\n",
        "\n",
        "![](figs/desmos-unstable.png){width=\"50%\"}\n",
        ":::\n",
        "\n",
        "## Wait, are linear controllers optimal?\n",
        "\n",
        "YES!\n",
        "\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots} && \\sum_{t=0}^{\\infty} \\gamma^t \\left( x_t^2 + \\rho u_t^2 \\right)\\\\\n",
        "\\text{where } && x_{t+1} = a x_t + b u_t \\\\\n",
        "&& \\require{enclose}\\enclose{horizontalstrike}[mathcolor=\"red\"]{\\color{black}{u_t = -k x_t}}\n",
        "\\end{align}\n",
        "\n",
        "## \n",
        "\n",
        "Define $$V^{\\star}(x) = \\min_{\\begin{align} u_0, & u_1, \\ldots \\\\ x_{t+1} &= a x_t + b u_t \\\\ x_0 &= x \\end{align}} \\sum_{t=0}^{\\infty} \\gamma^t \\left( x_t^2 + \\rho u_t^2 \\right)$$\n",
        "\n",
        "$$\\Downarrow \\text{(fact)}$$\n",
        "\n",
        "$$V^{\\star}(x) = P x^2$$\n",
        "\n",
        "The optimal value function is:\n",
        "\n",
        "1.  Quadratic\n",
        "2.  Parameterized by some $P > 0$\n",
        "\n",
        "## \n",
        "\n",
        "Bellman gives us a single variable problem: $$P x^2 = \\min_{u} \\left\\{ x^2 + \\rho u^2 + \\gamma P\\left( a x + b u \\right)^2 \\right\\}$$\n",
        "\n",
        "$$\\Downarrow \\text{solve } 0 = \\nabla_u \\left\\{ \\cdots \\right\\}$$\n",
        "\n",
        "$$u = -\\frac{\\gamma a b P}{\\rho + \\gamma b^2 P} x$$\n",
        "\n",
        "$$\\Downarrow$$\n",
        "\n",
        "Linear controller...but what is $P$?\n",
        "\n",
        "## \n",
        "\n",
        "Plug $u$ back into the Bellman equation to find: $$P = 1 + \\gamma a^2 P - \\frac{\\gamma^2 (a b P)^2}{\\rho + \\gamma b^2 P}$$\n",
        "\n",
        "A fixed point in $P$!\n",
        "\n",
        "Finding this fixed point in **parameter space** gives the optimal solution in value **function space**\n",
        "\n",
        "## \n",
        "\n",
        "<iframe src=\"https://www.desmos.com/calculator/n0zwcacqw7?embed\" width=\"500\" height=\"500\" style=\"border: 1px solid #ccc\" frameborder=\"0\">\n",
        "\n",
        "</iframe>\n",
        "\n",
        "## Recap lessons from RL section\n",
        "\n",
        "# Process control\n",
        "\n",
        "## \n",
        "\n",
        "A common control task is to bring a system to a constant value:\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "1.  Cruise control\n",
        "2.  Temperature\n",
        "3.  Concentrations\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "4.  Levels\n",
        "5.  Moisture\n",
        "6.  Etc...\n",
        ":::\n",
        "::::\n",
        "\n",
        "![](figs/feedback_diagram.png){height=\"200\"}\n",
        "\n",
        "# Linear policies\n",
        "\n",
        "# PID control\n",
        "\n",
        "*\"Based on a survey of over eleven thousand controllers in the refining, chemicals and pulp and paper industries, 97% of regulatory controllers utilize a PID feedback control algorithm.\"*\n",
        "\n",
        "-- @desborough2002IncreasingCustomer, also @aastrom2021feedback\n",
        "\n",
        "## \n",
        "\n",
        "::: {layout=\"[[1,1]]\"}\n",
        "![](figs/anim_impulse_u.gif)\n",
        "\n",
        "![](figs/anim_impulse_y.gif)\n",
        ":::\n",
        "\n",
        "The system settles at zero...how do we get it to settle somewhere else?\n",
        "\n",
        "## Proportional control\n",
        "\n",
        "$y_{sp}=$ desired value (sp = \"setpoint\")\n",
        "\n",
        "$y =$ measured value\n",
        "\n",
        "$e = y_{sp} - y$\n",
        "\n",
        "We want $e \\to 0$ as $t \\to \\infty$\n",
        "\n",
        "## Proportional control\n",
        "\n",
        "Consider the policy $$u = k_p e$$\n",
        "\n",
        "::: {layout=\"[[1,1]]\"}\n",
        "![](figs/anim_P_u.gif)\n",
        "\n",
        "![](figs/anim_P_y.gif)\n",
        ":::\n",
        "\n",
        "## Proportional-Integral control\n",
        "\n",
        "Consider the policy $$u(t) = k_p e(t) + k_i \\int_{0}^{t} e(\\tau) d \\tau$$\n",
        "\n",
        "::: {layout=\"[[1,1]]\"}\n",
        "![](figs/anim_PI_u.gif)\n",
        "\n",
        "![](figs/anim_PI_y.gif)\n",
        ":::\n",
        "\n",
        "## The magic of integral action\n",
        "\n",
        "1.  Assume that $k_p, k_i$ are chosen such that the system is stable\n",
        "2.  Then $u(t) \\to \\bar{u}$, $e(t) \\to \\bar{e}$\n",
        "3.  We can write $$ \\bar{u} = k_p \\bar{e} + k_i \\lim_{t \\to \\infty} \\int_{0}^{t} e(\\tau) d \\tau$$\n",
        "4.  The integral term must converge\n",
        "5.  Zero offset!\n",
        "\n",
        "## Proportional-Integral-Derivative control\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}\n",
        "![](figs/PID_Compensation_Animated.gif)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "**Proportional:** Go towards the setpoint\n",
        "\n",
        "**Integral:** Stay at the setpoint\n",
        "\n",
        "**Derivative:** Don't overshoot the setpoint\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: aside\n",
        "[PID controller (Wikipedia)](https://en.wikipedia.org/w/index.php?title=Proportional–integral–derivative_controller&oldid=1221531928)\n",
        ":::\n",
        "\n",
        "##  {auto-animate=\"true\"}\n",
        "\n",
        "PID controllers are great for tasks with a single input and single output.\n",
        "\n",
        "![](figs/feedback_SISO.png)\n",
        "\n",
        "##  {auto-animate=\"true\"}\n",
        "\n",
        "But often times we want to control a system with many input and output variables.\n",
        "\n",
        "![](figs/feedback_MIMO.png)\n",
        "\n",
        "## Can we tune controllers independently?\n",
        "\n",
        "Take two **independent** processes and design individual controllers\n",
        "\n",
        "![](figs/independent_loops.png)\n",
        "\n",
        "## Can we tune controllers independently?\n",
        "\n",
        "Those controllers can be disastrous if one process feeds into the other\n",
        "\n",
        "![](figs/coupled_loop.png)\n",
        "\n",
        "## Can we tune controllers independently?\n",
        "\n",
        "... sort of\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/astrom_pid.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "::: {layout=\"[[1,1,1],[1]]\"}\n",
        "![](figs/astrom.png){height=\"125\"}\n",
        "\n",
        "![](figs/johansson.png){height=\"125\"}\n",
        "\n",
        "![](figs/wang.png){height=\"125\"}\n",
        "\n",
        "![](figs/decoupled_screenshot.png){height=\"100\"}\n",
        ":::\n",
        ":::\n",
        "::::\n",
        "\n",
        "We need a scalable approach to handling loop interactions!\n",
        "\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "![](figs/Dunning-Kruger.jpg){.r-stretch fig-align=\"center\"}\n",
        "\n",
        "\n",
        "# LQR\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots} && \\sum_{t=0}^{\\infty} \\gamma^t \\left( x_{t}^{T} M x_t +  u_{t}^{T} R u_t \\right)\\\\\n",
        "\\text{where } && x_{t+1} = A x_t + B u_t \\\\\n",
        "\\end{align}\n",
        "\n",
        "-   **Linear:** Dynamics\n",
        "-   **Quadratic:** Cost (and value)\n",
        "-   **Regulator:** Keep state $x_t$ around $0$\n",
        "\n",
        "We already solved this in the scalar case!\n",
        "\n",
        "::: aside\n",
        "$M \\geq 0$, $R >$, $\\gamma \\in [0,1]$\n",
        ":::\n",
        "\n",
        "## General solution {auto-animate=\"true\"}\n",
        "\n",
        "1.  There is $P > 0$ such that $V^\\star (x) = x^T P x$. Then via the **Bellman equation** $$x_{t}^{T} P x_t = \\min_{u} \\left\\{ x_{t}^{T} M x_t +  u_{t}^{T} R u_t + \\gamma \\left( A x + B u \\right)^{T} P \\left( A x + B u \\right) \\right\\}$$\n",
        "\n",
        "## General solution {auto-animate=\"true\"}\n",
        "\n",
        "1.  Apply the **Bellman equation** with $V^\\star (x) = x^T P x$\n",
        "\n",
        "## General solution {auto-animate=\"true\"}\n",
        "\n",
        "1.  Apply the **Bellman equation** with $V^\\star (x) = x^T P x$\n",
        "2.  Enforce $$0 = \\nabla_u \\left\\{ \\cdots \\right\\}$$\n",
        "\n",
        "## General solution {auto-animate=\"true\"}\n",
        "\n",
        "1.  Apply the **Bellman equation** with $V^\\star (x) = x^T P x$\n",
        "2.  Enforce $0 = \\nabla_u \\left\\{ \\cdots \\right\\}$\n",
        "3.  Obtain $u = -K x$ where $$K = \\gamma \\left( R + \\gamma B^T P B \\right)^{-1} B^T P A$$\n",
        "\n",
        "## General solution {auto-animate=\"true\"}\n",
        "\n",
        "1.  Apply the **Bellman equation** with $V^\\star (x) = x^T P x$\n",
        "2.  Enforce $0 = \\nabla_u \\left\\{ \\cdots \\right\\}$\n",
        "3.  Obtain $u = - \\gamma \\left( R + \\gamma B^T P B \\right)^{-1} B^T P A x$\n",
        "4.  $P$ satisfies the **Discrete Algebraic Riccati Equation** $$P = M + \\gamma A^T P A - \\gamma^2 A^T P B \\left( R + \\gamma B^T P B \\right)^{-1} B^T P A$$\n",
        "\n",
        "## \n",
        "\n",
        "-   LQR is a tidy and globally optimal solution for controlling multivariable systems!\n",
        "\n",
        "-   It comes standard in any control systems library\n",
        "\n",
        "``` {.julia code-line-numbers=\"6-8\"}\n",
        "using ControlSystems\n",
        "\n",
        "Pd = c2d(ss(P),ts)\n",
        "A, B = Pd.A, Pd.B\n",
        "M, R = I, I\n",
        "\n",
        "K = lqr(Discrete, A, B, M, R)\n",
        "\n",
        "u(x,t)  = -K*x\n",
        "```\n",
        "\n",
        "::: aside\n",
        "Example is in the Julia package [ControlSystems.jl](https://juliacontrol.github.io/ControlSystems.jl/dev/). Other options include Matlab's [Control System Toolbox](https://www.mathworks.com/products/control.html), or [Python Control Systems Library](https://python-control.readthedocs.io/en/latest/index.html).\n",
        ":::\n",
        "\n",
        "## Aside about discounting\n",
        "\n",
        "Standard LQR solvers: $(A, B, M, R) \\to K$\n",
        "\n",
        "Discounted LQR: Use $$\\left( \\sqrt{\\gamma} A, B, M, \\frac{1}{\\gamma} R \\right)$$\n",
        "\n",
        "As $\\gamma \\to 0$...\n",
        "\n",
        "-   \"Ignore the state transition matrix\"\n",
        "-   \"Apply infinite weight to control actions\"\n",
        "\n",
        "::: {.r-fit-text}\n",
        "Unstable controller\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/lqr_disturbance.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "- The controller quickly brings the system to $0$\n",
        "- A random disturbance in $u_2$ occurs at $t = 15$, affecting $y_1, y_2$\n",
        "- The controller brings $y_1$ and $y_2$ back to equilibrium\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Globally optimal?\n",
        "\n",
        "All systems are subject to constraints:\n",
        "\n",
        "-   Finite resources & money\n",
        "-   Limited actuation\n",
        "\n",
        "LQR assumes:\n",
        "\n",
        "-   Any control action is permissible\n",
        "-   Any intermediate state is acceptable\n",
        "\n",
        "## Globally optimal?\n",
        "\n",
        "Consider the system $$x_{t+1} = \\begin{bmatrix} 1 & 1\\\\ 0 & 1 \\end{bmatrix} x_t + \\begin{bmatrix} 1 \\\\ 0.5 \\end{bmatrix} u_t$$\n",
        "\n",
        "Unstable $\\to$ control is needed to achieve equilibrium\n",
        "\n",
        "## Globally optimal?\n",
        "\n",
        "LQR, business as usual\n",
        "\n",
        "![](figs/lqr_unconstrained.png){fig-align=\"center\"}\n",
        "\n",
        "## Globally optimal?\n",
        "\n",
        "Suppose only actions in the set $\\left\\{ u \\colon \\lVert u \\rVert_{\\infty} \\leq 1 \\right\\}$ are possible\n",
        "\n",
        "And we want the states to stay in $\\left\\{ x \\colon \\lVert x \\rVert_{\\infty} \\leq 5 \\right\\}$\n",
        "\n",
        "![](figs/lqr_constrained.png){fig-align=\"center\"}\n",
        "\n",
        "## Globally optimal?\n",
        "\n",
        "The actions can start out feasible, then become infeasible later\n",
        "\n",
        "![](figs/lqr_constrained_later.png){fig-align=\"center\"}\n",
        "\n",
        "## Locally optimal\n",
        "\n",
        "![](figs/lqr_constrainedSS.png){height=\"400\" fig-align=\"center\"}\n",
        "\n",
        "::: aside\n",
        "See @borrelli2017predictive or [slides](https://kwonlecture.snu.ac.kr/wp-content/uploads/2019/08/MPC_Course_v1.pdf)\n",
        ":::\n",
        "\n",
        "# Nonlinear policies\n",
        "\n",
        "##  {auto-animate=\"true\"}\n",
        "\n",
        "::: {style=\"margin-top: 300px; margin-left: 100px;\"}\n",
        "What if a controller could anticipate an obstacle?\n",
        ":::\n",
        "\n",
        "##  {auto-animate=\"true\"}\n",
        "\n",
        "What if a controller could anticipate an obstacle?\n",
        "\n",
        "![](figs/lane_change.png){.r-stretch fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## \n",
        "\n",
        "From this...\n",
        "\n",
        "![](figs/anim_dip_no-obstacle.gif){.r-stretch fig-align=\"center\"}\n",
        "\n",
        "\n",
        "\n",
        "## \n",
        "\n",
        "To this...\n",
        "\n",
        "![](figs/anim_dip.gif){.r-stretch fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## Anticipating constraints is inherently nonlinear\n",
        "\n",
        "Suppose $x_t$ is \"close\" to upper constraint $c$:\n",
        "\n",
        "- Take conservative actions near constraint\n",
        "- More freedom away from constraints\n",
        "\n",
        "LQR:\n",
        "\n",
        "- Follow $u_t = -K x_t$ no matter what\n",
        "\n",
        "\n",
        "\n",
        "# Model predictive control\n",
        "\n",
        "\n",
        "## Problem formulation\n",
        "\n",
        "\n",
        "MPC is a common-sense strategy of making decisions by predicting the future\n",
        "\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots u_{N-1}} \\quad & \\sum_{t=0}^{N-1} x_{t}^{T} M x_t +  u_{t}^{T} R u_t \\\\\n",
        "\\text{where } \\quad & x_{t+1} = A x_t + B u_t \\\\\n",
        "& x_t \\in \\mathcal{X} \\\\\n",
        "& u_t \\in \\mathcal{U}\n",
        "\\end{align}\n",
        "\n",
        "::: aside\n",
        "$\\mathcal{X}, \\mathcal{U}$ are often box constraints: $u_{\\min} \\leq u_t \\leq u_{\\max} \\forall t$.\n",
        "Like LQR, $M \\geq 0, R > 0$.\n",
        ":::\n",
        "\n",
        "\n",
        "## Problem formulation\n",
        "\n",
        "Applying the optimal inputs $u_0^\\star, u_1^\\star, \\ldots$ is an **open-loop** strategy\n",
        "\n",
        "- Model errors compound\n",
        "- Unexpected disturbances will go unchecked\n",
        "- Will need to solve the MPC problem after $N$ time steps anyway\n",
        "\n",
        "\n",
        "\n",
        "## The receding horizon idea\n",
        "\n",
        "At each time step, re-initialize the MPC problem with the state $s_t$ from the environment:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots u_{N-1}} \\quad & \\sum_{k=0}^{N-1} x_{k}^{T} M x_k +  u_{}^{T} R u_k \\\\\n",
        "\\text{where } \\quad & \\bbox[#FF9999,2pt]{x_0 = s_t} \\\\\n",
        "& x_{k+1} = A x_k + B u_k \\\\\n",
        "& x_k \\in \\mathcal{X} \\\\\n",
        "& u_k \\in \\mathcal{U}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "## The receding horizon idea\n",
        "\n",
        "MPC controller\n",
        "\n",
        "1. Initialize state $x_0 = s$\n",
        "2. Solve the MPC optimization problem\n",
        "3. Apply $u_0^\\star$ to the system\n",
        "4. Update system state $s \\leftarrow s'$\n",
        "\n",
        "## The receding horizon idea\n",
        "\n",
        "\n",
        "![](figs/receding_horizon.png){width=500 fig-align=\"center\"}\n",
        "\n",
        "::: aside\n",
        "See @borrelli2017predictive, Chapter 12\n",
        ":::\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "::: {layout=\"[[1,1]]\" layout-valign=\"bottom\"}\n",
        "![](figs/mpc_closedlooptraj1.png){height=350}\n",
        "\n",
        "![](figs/mpc_closedlooptraj2.png){height=400}\n",
        ":::\n",
        "\n",
        "Solid - realized closed-loop trajectories\n",
        "\n",
        "Dash - predicted trajectory\n",
        "\n",
        "::: aside\n",
        "See @borrelli2017predictive, Chapter 12\n",
        ":::\n",
        "\n",
        "##\n",
        "\n",
        "::: {layout=\"[[1,1],[1,1]]\" layout-halign=\"center\"}\n",
        "LQR\n",
        "\n",
        "MPC\n",
        "\n",
        "![](figs/lqr_constrainedSS.png){height=\"400\"}\n",
        "\n",
        "![](figs/mpc_constrainedSS.png){height=\"400\"}\n",
        ":::\n",
        "\n",
        "\n",
        "## MPC control law is nonlinear\n",
        "\n",
        "\n",
        "![](figs/mpc_PWA.png)\n",
        "\n",
        "\n",
        "## Why? {.center}\n",
        "\n",
        "MPC = Multi-parametric quadratic programming\n",
        "\n",
        "## {auto-animate=\"true\"}\n",
        "\n",
        "\n",
        "Original problem:\n",
        "\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots u_{N-1}} \\quad & \\sum_{k=0}^{N-1} x_{k}^{T} M x_k +  u_{}^{T} R u_k \\\\\n",
        "\\text{where } \\quad & x_0 = s_t \\\\\n",
        "& x_{k+1} = A x_k + B u_k \\\\\n",
        "& x_k \\in \\mathcal{X} \\\\\n",
        "& u_k \\in \\mathcal{U}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "## {auto-animate=\"true\"}\n",
        "\n",
        "Unroll the dynamics:\n",
        "\n",
        "::: {.r-stack}\n",
        "$$x_{k+1} = A x_k + B u_k$$\n",
        ":::\n",
        "\n",
        "## {auto-animate=\"true\"}\n",
        "\n",
        "Unroll the dynamics:\n",
        "\n",
        "\n",
        "::: {.r-stack}\n",
        "\n",
        "::: {.fragment .fade-in-then-out}\n",
        "$$x_1 = B u_0$$\n",
        ":::\n",
        "\n",
        "::: {.fragment .fade-in-then-out}\n",
        "$$x_2 = B u_1 + A B u_0$$\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "::: {.fragment .fade-in-then-out}\n",
        "$$x_3 = B u_2 + A B u_1 + A^2 B u_0$$\n",
        ":::\n",
        "\n",
        "::: {.fragment .fade-in-then-out}\n",
        "$$x_n =\n",
        "\\begin{bmatrix}\n",
        "B & A B & \\cdots & A^{N-1} B\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "u_{N-1} \\\\ u_{N-2} \\\\ \\vdots \\\\ u_0\n",
        "\\end{bmatrix}$$\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: aside\n",
        "$x_{k+1} = A x_k + B u_k$ and $x_0 = 0$ for ease\n",
        ":::\n",
        "\n",
        "## {auto-animate=\"true\"}\n",
        "\n",
        "Algebra:\n",
        "\n",
        "\\begin{align}\n",
        "\\min_{U} \\quad & \\frac{1}{2} U^T H U + s_t^T F U \\\\\n",
        "\\text{where } \\quad & G U \\leq W + E s_t \\\\\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "See @bemporad2002ExplicitLinear\n",
        "\n",
        "\n",
        "## {auto-animate=\"true\"}\n",
        "\n",
        "KKT conditions:\n",
        "\n",
        "$U^\\star$ and associated Lagrange multipliers are affine in $s_t$\n",
        "\n",
        "\n",
        "![](figs/mpc_explicit.png)\n",
        "\n",
        "\n",
        "## MPC = Continuous piecewise affine\n",
        "\n",
        "\n",
        "::: {layout=\"[[1,0.2,1]]\" layout-valign=\"center\"}\n",
        "![MPC control law](figs/mpc_surface.png){height=250}\n",
        "\n",
        "<div style=\"font-size:150px\">&#x1F91D;</div>\n",
        "\n",
        "![ReLU DNN](figs/relu_surface.png){height=250}\n",
        ":::\n",
        "\n",
        "::: aside\n",
        "See @bemporad2002ExplicitLinear, @karg2020EfficientRepresentation, @montufar2014number\n",
        ":::\n",
        "\n",
        "\n",
        "## Stability issues {.center}\n",
        "\n",
        "*\"In the engineering literature it is often assumed (tacitly and incorrectly) that a system with optimal control law is necessarily stable.\"*\n",
        "\n",
        "-- @kalman1960contributions\n",
        "\n",
        "## {.center}\n",
        "\n",
        "\n",
        "Repeatedly implementing a finite horizon solution on an infinite horizon problem leads to \"surprises\"\n",
        "\n",
        "- Infeasibility\n",
        "- Instability\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "How can we solve an infinite horizon optimal control problem with finite resources?\n",
        "\n",
        "\n",
        "1. Terminal constraint\n",
        "2. Terminal cost\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots u_{N-1}} \\quad & \\sum_{k=0}^{N-1} \\left( x_{k}^{T} M x_k +  u_{k}^{T} R u_k \\right) + \\bbox[#FF9999,2pt]{x_{N}^{T} P x_{N}}\\\\\n",
        "\\text{where } \\quad & x_0 = s_t \\\\\n",
        "& x_{k+1} = A x_k + B u_k \\\\\n",
        "& x_k \\in \\mathcal{X}, u_k \\in \\mathcal{U} \\\\\n",
        "& \\bbox[#FF9999,2pt]{x_{N} \\in \\mathcal{X}_f}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "## {.center}\n",
        "\n",
        "How to design terminal cost?\n",
        "\n",
        "LQR!\n",
        "\n",
        "\n",
        "1. Obtain $P$ by solving $\\texttt{lqr}(A, B, M, R)$\n",
        "2. Embed a fictitious LQR controller $u_t = -K x_t$ into MPC after $N_c$ time steps\n",
        "\n",
        "\n",
        "## Final objective\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots u_{N_c - 1}} \\quad & \\sum_{k=0}^{N-1} \\left( x_{k}^{T} M x_k +  u_{k}^{T} R u_k \\right) + x_{N}^{T} P x_{N}\\\\\n",
        "\\text{where }\\quad & x_0 = s_t \\\\\n",
        "& x_{k+1} = A x_k + B u_k \\\\\n",
        "& x_k \\in \\mathcal{X},\\quad u_k \\in \\mathcal{U} \\\\\n",
        "& u_{k} = -K x_k,\\quad N_c \\leq k < N\n",
        "\\end{align}\n",
        "\n",
        "::: {.r-fit-text}\n",
        "Mimic infinite horizon behavior\n",
        ":::\n",
        "\n",
        "## \n",
        "\n",
        "![](figs/cl.gif){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "![](figs/Chaotic+Grad+Student+HiRes.jpg){.r-stretch fig-align=\"center\"}\n",
        "\n",
        "\n",
        "# MPC + RL\n",
        "\n",
        "\n",
        "![](figs/rl_mpc_tree.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "::: aside\n",
        "@arroyo2022ReinforcedModel\n",
        ":::\n",
        "\n",
        "\n",
        "## Main motivation\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "MPC\n",
        "\n",
        "\n",
        "\n",
        "<ul style=\"list-style-type: '+ ';\">\n",
        "  <li>Safety by design</li>\n",
        "  <li>Modularity</li>\n",
        "</ul>\n",
        "\n",
        "<ul style=\"list-style-type: '- ';\">\n",
        "  <li>Manual design</li>\n",
        "  <li>Rigid</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "RL\n",
        "\n",
        "<ul style=\"list-style-type: '+ ';\">\n",
        "  <li>Model-free</li>\n",
        "  <li>Flexible objectives</li>\n",
        "</ul>\n",
        "\n",
        "<ul style=\"list-style-type: '- ';\">\n",
        "  <li>Safety constraints</li>\n",
        "  <li>Slowish learning</li>\n",
        "</ul>\n",
        "\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "\n",
        "## MPC + value function\n",
        "\n",
        "::: {.r-stack}\n",
        "::: {.fragment .fade-in-then-out}\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots u_{N - 1}} \\quad & \\sum_{k=0}^{N-1} \\ell(x_k, u_k) \\phantom{\\quad + \\overbrace{V_{\\theta}(x_N)}^{\\text{Learnable residual}}}\\\\\n",
        "\\text{where }\\quad & x_0 = s_t \\\\\n",
        "& x_{k+1} = f(x_k, u_k) \\\\\n",
        "& \\underbrace{x_k \\in \\mathcal{X},\\quad u_k \\in \\mathcal{U}}_{\\text{Prior engineering}} \\\\\n",
        "\\end{align}\n",
        ":::\n",
        "\n",
        "::: {.fragment .fade-in-then-out}\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots u_{N - 1}} \\quad & \\sum_{k=0}^{N-1} \\ell(x_k, u_k)\\quad + \\overbrace{V_{\\theta}(x_N)}^{\\text{Learnable residual}}\\\\\n",
        "\\text{where }\\quad & x_0 = s_t \\\\\n",
        "& x_{k+1} = f(x_k, u_k) \\\\\n",
        "& \\underbrace{x_k \\in \\mathcal{X},\\quad u_k \\in \\mathcal{U}}_{\\text{Prior engineering}} \\\\\n",
        "\\end{align}\n",
        ":::\n",
        "\n",
        "::: {.fragment .fade-in}\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots u_{N - 1}} & \\sum_{k=0}^{N-1} \\ell(x_k, u_k) + V_{\\theta}(x_N)\\\\\n",
        "\\text{where }\\quad & x_0 = s_t \\\\\n",
        "& x_{k+1} = f(x_k, u_k) \\\\\n",
        "& x_k \\in \\mathcal{X},\\quad u_k \\in \\mathcal{U} \\\\\n",
        "\\end{align}\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "# Implementation\n",
        "\n",
        "\n",
        "\n",
        "## MPC frameworks \n",
        "\n",
        "\n",
        "::: {layout=\"[[1,1,1],[1,1],[1,1]]\" layout-valign=\"bottom\"}\n",
        "\n",
        "![@open2020](figs/package_optimization-engine.png){height=100}\n",
        "\n",
        "![@fiedler2023DompcFAIR](figs/package_dompc.png){height=150}\n",
        "\n",
        "![[Model Predictive Control Toolbox](https://www.mathworks.com/products/model-predictive-control.html)](figs/package_mathworks.png){height=100}\n",
        "\n",
        "![@verschueren2022AcadosModulara](figs/package_acados.png){height=50}\n",
        "\n",
        "![@englert2019SoftwareFramework](figs/package_grampc.png){height=50}\n",
        "\n",
        "![@chen2019MATMPCMATLAB](figs/package_matmpc.png){height=50}\n",
        "\n",
        "![[MPC Tools](https://sites.engineering.ucsb.edu/~jbraw/software/mpctools/examples/index.html)](figs/package_mpctools.png){height=50}\n",
        ":::\n",
        "\n",
        "## MPC frameworks\n",
        "\n",
        "[do-mpc](https://www.do-mpc.com/en/latest/):\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column}\n",
        "- Open source\n",
        "- Modular\n",
        ":::\n",
        "\n",
        "::: {.column}\n",
        "- Python interface\n",
        "- Fast\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {layout=\"[[1,1]]\" layout-valign=\"center\"}\n",
        "\n",
        "![@fiedler2023DompcFAIR](figs/package_dompc.png){height=150}\n",
        "\n",
        "![@Andersson2019](figs/casadi_logo.png){height=150}\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Example: Triple mass spring system \n",
        "\n",
        "\n",
        "\n",
        "::: {layout=\"[[1,0.2,1]]\" layout-valign=\"center\"} \n",
        "![](figs/anim_disc_3d_uncontrolled.gif)\n",
        "\n",
        "$$\\rightarrow$$\n",
        "\n",
        "![](figs/anim_disc_3d_ctrl_motor.gif)\n",
        ":::\n",
        "\n",
        "::: aside\n",
        "See [notebook](https://github.com/do-mpc/do-mpc/blob/master/documentation/source/getting_started.ipynb) from [do-mpc](https://www.do-mpc.com/en/latest/index.html) for full code samples. We only show snippets of the key destinations.\n",
        ":::\n",
        "\n",
        "## Create model\n",
        "\n",
        "``` {.python code-line-numbers=\"7,9,10\"}\n",
        "import do_mpc\n",
        "from casadi import *\n",
        "\n",
        "model_type = 'continuous' # either 'discrete' or 'continuous'\n",
        "model = do_mpc.model.Model(model_type)\n",
        "\n",
        "dphi = model.set_variable(var_type='_x', var_name='dphi', shape=(3,1))\n",
        "# Two states for the desired (set) motor position:\n",
        "phi_m_1_set = model.set_variable(var_type='_u', var_name='phi_m_1_set')\n",
        "phi_m_2_set = model.set_variable(var_type='_u', var_name='phi_m_2_set')\n",
        "```\n",
        "\n",
        "\n",
        "![](figs/triple_mass_spring.png){height=200 fig-align=\"center\"}\n",
        "\n",
        "## Right-hand-side equation\n",
        "\n",
        "Define the states, inputs, parameters, and function composing an ODE $$\\dot{x} = f(x, u)$$\n",
        "\n",
        "``` {.python code-line-numbers=\"1,14,15\"}\n",
        "Theta_1 = model.set_variable('parameter', 'Theta_1') \n",
        "Theta_2 = model.set_variable('parameter', 'Theta_2')\n",
        "Theta_3 = model.set_variable('parameter', 'Theta_3')\n",
        "\n",
        "c = np.array([2.697,  2.66,  3.05, 2.86])*1e-3\n",
        "d = np.array([6.78,  8.01,  8.82])*1e-5\n",
        "\n",
        "dphi_next = vertcat(\n",
        "    -c[0]/Theta_1*(phi_1-phi_1_m)-c[1]/Theta_1*(phi_1-phi_2)-d[0]/Theta_1*dphi[0],\n",
        "    -c[1]/Theta_2*(phi_2-phi_1)-c[2]/Theta_2*(phi_2-phi_3)-d[1]/Theta_2*dphi[1],\n",
        "    -c[2]/Theta_3*(phi_3-phi_2)-c[3]/Theta_3*(phi_3-phi_2_m)-d[2]/Theta_3*dphi[2],\n",
        ")\n",
        "\n",
        "model.set_rhs('dphi', dphi_next)\n",
        "model.setup()\n",
        "```\n",
        "\n",
        "## Create controller\n",
        "\n",
        "``` {.python code-line-numbers=\"1,9\"}\n",
        "mpc = do_mpc.controller.MPC(model)\n",
        "\n",
        "setup_mpc = {\n",
        "    'n_horizon': 20,\n",
        "    't_step': 0.1,\n",
        "    'n_robust': 1,\n",
        "    'store_full_solution': True,\n",
        "}\n",
        "mpc.set_param(**setup_mpc)\n",
        "```\n",
        "(Defining constraints, the objective function, and even uncertain parameters, all follow a similar workflow\n",
        ")\n",
        "\n",
        "``` {.python}\n",
        "mpc.setup()\n",
        "```\n",
        "\n",
        "## Define simulator\n",
        "\n",
        "Either use the same model inside the MPC or define a different model to simulate the \"true\" system:\n",
        "\n",
        "- Simplified MPC model\n",
        "- Complex \"true\" simulator model\n",
        "\n",
        "\n",
        "``` {.python}\n",
        "simulator = do_mpc.simulator.Simulator(model)\n",
        "```\n",
        "\n",
        "## Run the control loop\n",
        "\n",
        "``` {.python}\n",
        "for i in range(20):\n",
        "    u0 = mpc.make_step(x0)\n",
        "    x0 = simulator.make_step(u0)\n",
        "```\n",
        "![](figs/anim_disc.gif){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "\n",
        "Creating these gifs is easy with do-mpc's `Graphics` and `Data` modules\n",
        "\n",
        "``` {.python}\n",
        "mpc_graphics = do_mpc.graphics.Graphics(mpc.data)\n",
        "sim_graphics = do_mpc.graphics.Graphics(simulator.data)\n",
        "```\n",
        "\n",
        "![](figs/do_mpc_api.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## do-mpc summary\n",
        "\n",
        "![](figs/do_mpc_flow_sheet.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## RL frameworks \n",
        "\n",
        "\n",
        "\n",
        "::: {layout=\"[[1,1,1],[1,1],[1,1]]\" layout-valign=\"bottom\"}\n",
        "![@raffin2021StableBaselines3Reliable](figs/package_stable.png){height=100}\n",
        "\n",
        "![@huang2022CleanRLHighquality](figs/package_cleanrl.png)\n",
        "\n",
        "![[Spinning Up](https://github.com/openai/spinningup)](figs/package_spinup.png){height=100}\n",
        "\n",
        "![@hoffman2022AcmeResearch](figs/package_acme.png){height=50}\n",
        "\n",
        "![@bou2023TorchRLDatadriven](figs/package_torchrl.png){height=50}\n",
        "\n",
        "![@weng2022TianshouHighly](figs/package_tianshou.png){height=50}\n",
        "\n",
        "![@liang2018RLlibAbstractions](figs/package_rllib.png){height=50}\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "<div style=\"position:absolute; right:0;\"> <small>(...A really long list [here](https://github.com/wwxFromTju/awesome-reinforcement-learning-lib))</small> </div>\n",
        "\n",
        "\n",
        "## RL frameworks \n",
        "\n",
        "[CleanRL](https://docs.cleanrl.dev):\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column}\n",
        "- Self-contained implementations\n",
        "- Rapid prototyping\n",
        ":::\n",
        "\n",
        "::: {.column}\n",
        "- Thorough documentation and benchmarking\n",
        "- `Gym` for environments and `wandb` for tracking\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {layout=\"[[1,1]]\" layout-valign=\"center\"}\n",
        "\n",
        "![@towers_gymnasium_2023,@brockman2016OpenAIGym](figs/package_gymnasium.png){height=100}\n",
        "\n",
        "![@biewald2020experiment](figs/package_wandb.png){height=100}\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column}\n",
        "CleanRL:\n",
        "\n",
        "- 1 algorithm gets 1 file\n",
        "- Read, learn, and modify in a linear fashion\n",
        "- 300-400 lines of code\n",
        "  - Including all utilities\n",
        ":::\n",
        "\n",
        "::: {.column}\n",
        "Modular libraries:\n",
        "\n",
        "![](figs/sb3_loc.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "## {.center}\n",
        "\n",
        "Like with the MPC packages, we're choosing the best one for our purposes---the other ones are absolutely worth looking into!\n",
        "\n",
        "## Break\n",
        "\n",
        "![[Open RL Benchmark](https://wandb.ai/openrlbenchmark/openrlbenchmark/reports/Atari-CleanRL-s-DQN--VmlldzoxNjk3NjYx) [@huang2024OpenRL]](videos/atari_breakout.gif){height=350 fig-align=\"center\"}\n",
        "\n",
        "\n",
        "\n",
        "## Combining RL and MPC\n",
        "\n",
        "Recall what we're after:\n",
        "\n",
        "\\begin{align}\n",
        "\\min_{u_0, u_1, \\ldots u_{N - 1}} \\quad & \\sum_{k=0}^{N-1} \\ell(x_k, u_k) \\quad + \\overbrace{V_{\\theta}(x_N)}^{\\text{Learnable residual}}\\\\\n",
        "\\text{where }\\quad & x_0 = s_t \\\\\n",
        "& x_{k+1} = f(x_k, u_k) \\\\\n",
        "& \\underbrace{x_k \\in \\mathcal{X},\\quad u_k \\in \\mathcal{U}}_{\\text{Prior engineering}} \\\\\n",
        "\\end{align}\n",
        "\n",
        "## Combining RL and MPC\n",
        "\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column}\n",
        "CleanRL:\n",
        "\n",
        "1. Value function learning\n",
        "2. Environment implementation\n",
        ":::\n",
        "\n",
        "::: {.column}\n",
        "do-mpc:\n",
        "\n",
        "1. Optimization module\n",
        "2. Simulation\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "## 1. RL value function in do-mpc\n",
        "\n",
        "Value function training\n",
        "\n",
        "``` {.python code-line-numbers=\"1,7,8,13,18\"}\n",
        "data = rb.sample(args.batch_size)\n",
        "with torch.no_grad():\n",
        "    next_state_actions, next_state_log_pi, _ = actor.get_action(data.next_observations)\n",
        "    qf1_next_target = qf1_target(data.next_observations, next_state_actions)\n",
        "    qf2_next_target = qf2_target(data.next_observations, next_state_actions)\n",
        "    min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi\n",
        "    next_q_value = (data.rewards.flatten() + \n",
        "        (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1))\n",
        "\n",
        "qf1_a_values = qf1(data.observations, data.actions).view(-1)\n",
        "qf2_a_values = qf2(data.observations, data.actions).view(-1)\n",
        "qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
        "qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
        "qf_loss = qf1_loss + qf2_loss\n",
        "\n",
        "# optimize the model\n",
        "q_optimizer.zero_grad()\n",
        "qf_loss.backward()\n",
        "q_optimizer.step()\n",
        "```\n",
        "\n",
        "## 1. RL value function in do-mpc\n",
        "\n",
        "Export value function to do-mpc:\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column  width=\"70%\"}\n",
        "1. Export PyTorch model to [ONNX](https://onnx.ai) using `torch.onnx.export()`\n",
        "2. Export ONNX model to CasADi `do_mpc.sysid.ONNXConversion()`\n",
        "\n",
        "![](figs/onnx.png){width=200}\n",
        ":::\n",
        "\n",
        "::: {.column  width=\"30%\"}\n",
        "![@fiedler2023DompcFAIR](figs/do_mpc_elements.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "## 1. RL value function in do-mpc\n",
        "\n",
        "Implement MPC with terminal value function:\n",
        "\n",
        "``` {.python code-line-numbers=\"1,3,6,12,17\"}\n",
        "mpc = do_mpc.controller.MPC(model)\n",
        "\n",
        "mpc.settings.n_horizon = 1\n",
        "lterm = model.aux['cost'] \n",
        "\n",
        "terminal_converter = do_mpc.sysid.ONNXConversion(value_onnx)\n",
        "def terminal_casadi(x):\n",
        "    terminal_converter.convert(x = x.T, goal=np.zeros(x.T.shape))\n",
        "    return terminal_converter['terminal_cost']\n",
        "mterm = -terminal_casadi(model.x['x'])\n",
        "\n",
        "mpc.set_objective(lterm=lterm, mterm=mterm)\n",
        "\n",
        "mpc.bounds['lower','_u','u'] = -0.5\n",
        "mpc.bounds['upper','_u','u'] =  0.5\n",
        "\n",
        "mpc.setup()\n",
        "```\n",
        "\n",
        "## 1. RL value function in do-mpc\n",
        "\n",
        "Run MPC + value function controller in CleanRL:\n",
        "\n",
        "``` {.python code-line-numbers=\"2,11\"}\n",
        "x0 = estimator.make_step(obs[\"observation\"])\n",
        "action = mpc.make_step(x0)\n",
        "\n",
        "# A quick way of incorporating exploration\n",
        "with torch.no_grad():\n",
        "    if global_step % args.policy_frequency == 0: # optional: add some delay between disturbances (exploration)\n",
        "        noise = actor._explore_noise(obs).numpy()\n",
        "        action += noise\n",
        "        action = np.float32(action.clip(env.action_space.low, env.action_space.high))\n",
        "\n",
        "next_ob, reward, done, info = env.step(action)\n",
        "```\n",
        "\n",
        "Next, what is `env.step()`?\n",
        "\n",
        "\n",
        "## 2. Gym wrapper for do-mpc simulation\n",
        "\n",
        "Zooming out a bit, a basic RL loop looks like this:\n",
        "\n",
        "``` {.python code-line-numbers=\"3,7,8\"}\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()\n",
        "```\n",
        "\n",
        "##\n",
        "\n",
        "Environments follow a basic blueprint:\n",
        "\n",
        "``` {.python code-line-numbers=\"9-11\"}\n",
        "from gymnasium import spaces\n",
        "\n",
        "class CustomEnv(gym.Env):\n",
        "  def __init__(self, arg1, arg2, ...):\n",
        "    super(CustomEnv, self).__init__()\n",
        "    self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n",
        "    self.observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
        "\n",
        "  def step(self, action):\n",
        "    ...\n",
        "    return observation, reward, done, info\n",
        "  def reset(self):\n",
        "    ...\n",
        "    return observation\n",
        "  def render(self, mode='human'):\n",
        "    ...\n",
        "  def close (self):\n",
        "    ...\n",
        "```\n",
        "\n",
        "##\n",
        "\n",
        "Create Gym environment that queries do-mpc simulation:\n",
        "\n",
        "``` {.python code-line-numbers=\"5,6,14\"}\n",
        "class DoMPCEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Gym environment that uses do-mpc for carrying out simulations\n",
        "    \"\"\"\n",
        "    def __init__(self, simulator:do_mpc.simulator.Simulator, \n",
        "                    num_steps=100):        \n",
        "        super().__init__()\n",
        "        ...\n",
        "\n",
        "    def step(self, action):\n",
        "        # simplified version --- hides some processing steps\n",
        "\n",
        "        self.t += 1\n",
        "        self.state = self.simulator.make_step(action)\n",
        "        info = self._get_info()\n",
        "        reward, terminated, truncated = info[\"reward\"], info[\"distance\"]<0.01, self.t == self.num_steps\n",
        "\n",
        "        return self.state, reward, terminated, truncated, info\n",
        "    ...\n",
        "```\n",
        "\n",
        "## Example: Oscillating masses\n",
        "\n",
        "State: Position and velocity of each mass\n",
        "\n",
        "Action: Force applied to $m_2$\n",
        "\n",
        "MPC cost: $\\lVert x \\rVert^2$\n",
        "\n",
        "Reward: 0 if  $\\lVert x \\rVert_{\\infty} \\leq \\epsilon$; $-1$ otherwise\n",
        "\n",
        "![](figs/oscillating_masses.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## \n",
        "\n",
        "\n",
        "![](figs/reward_mass.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## \n",
        "\n",
        "\n",
        "![](figs/reward_mass_HER.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "::: {layout=\"[[1,1,1], [1,1,1], [1,1,1]]\"}\n",
        "<div>\n",
        "\n",
        "</div>\n",
        "\n",
        "\"Bad\" model\n",
        "\n",
        "\"Good\" model\n",
        "\n",
        "<div>\n",
        "\n",
        "\\\n",
        "\\\n",
        "No relabeling\n",
        "\n",
        "</div>\n",
        "\n",
        "![](figs/masses-noHER-badmodel.gif)\n",
        "\n",
        "![](figs/masses-noHER-goodmodel.gif)\n",
        "\n",
        "\n",
        "\n",
        "<div>\n",
        "\n",
        "\\\n",
        "\\\n",
        "Relabeling\n",
        "\n",
        "</div>\n",
        "\n",
        "![](figs/masses-HER-badmodel.gif)\n",
        "\n",
        "![](figs/masses-HER-goodmodel.gif)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## Studies in the literature\n",
        "\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "![](figs/Automation+Paradox.jpg){.r-stretch fig-align=\"center\"}\n",
        "\n",
        "\n",
        "# Implementation\n",
        "\n",
        "\n",
        "\n",
        "# Addendum\n",
        "\n",
        "## General MPC problem formulation\n",
        "\n",
        "## Estimation, offset-free tracking\n",
        "\n",
        "## Differentiable MPC\n",
        "\n",
        "# Industrial control: theory vs practice\n",
        "\n",
        "# Thank you\n",
        "\n",
        "## Credits\n",
        "\n",
        "\n",
        "\n",
        "## Slides and code\n",
        "\n",
        "![](figs/qr-github.jpeg){fig-align=\"center\"}\n",
        "\n",
        "::: aside\n",
        "Link: [https://github.com/NPLawrence/upperbound-tutorial](https://github.com/NPLawrence/upperbound-tutorial)\n",
        ":::\n",
        "\n",
        "## References\n",
        "\n",
        "::: {#refs}\n",
        ":::"
      ],
      "id": "c9cb004d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}